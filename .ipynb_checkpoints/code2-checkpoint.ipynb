{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import warnings\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'input'\n",
    "Metro_roadMap = pd.read_csv(path + '/Metro_roadMap.csv')\n",
    "Metro_roadMap.rename(columns={'Unnamed: 0':'stationID'},inplace=True)\n",
    "testB_record  = pd.read_csv(path + '/Metro_testB/testB_record_2019-01-26.csv')\n",
    "testB_submit  = pd.read_csv(path + '/Metro_testB/testB_submit_2019-01-27.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取测试集日期\n",
    "testB_submit['time']    = pd.to_datetime(testB_submit['startTime'])\n",
    "test_day                = int(testB_submit['time'].dt.day.mean())\n",
    "test_week               = int(testB_submit['time'].dt.dayofweek.mean())\n",
    "del testB_submit['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_week = 3\n",
    "test_day = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间转化\n",
    "def trans_time_10_minutes(x):\n",
    "    x_split = x.split(':')\n",
    "    x_part1 = x_split[0]\n",
    "    x_part2 = int(x_split[1]) // 10\n",
    "    if x_part2 == 0:\n",
    "        x_part2 = '00'\n",
    "    else:\n",
    "        x_part2 = str(x_part2 * 10)\n",
    "    return x_part1 + ':' + x_part2 + ':00'\n",
    "\n",
    "def get_base_features(df_):\n",
    "    \n",
    "    df = df_.copy()\n",
    "    \n",
    "    # base time\n",
    "    df['time']    = pd.to_datetime(df['time'])\n",
    "    df['day' ]    = df['time'].dt.day  \n",
    "    df['week']    = df['time'].dt.dayofweek \n",
    "    df['hour']    = df['time'].dt.hour \n",
    "    df['minute']  = df['time'].dt.minute \n",
    "    df['ten_minutes_in_day'] = df['hour'] * 6 + df['time'].dt.minute // 10 \n",
    "    \n",
    "    df['time_ten_minutes'] = df['time'].astype(str).apply(lambda x: trans_time_10_minutes(x))\n",
    "    \n",
    "    # count,sum\n",
    "    result = df.groupby(['stationID', 'time_ten_minutes', 'week', 'day', 'hour', 'ten_minutes_in_day'\\\n",
    "                        ]).status.agg(['count', 'sum']).reset_index()\n",
    "    \n",
    "    # in,out\n",
    "    result['inNums']  = result['sum']\n",
    "    result['outNums'] = result['count'] - result['sum']\n",
    "    \n",
    "    result.fillna(0, inplace=True)\n",
    "    del result['sum'],result['count']\n",
    "    \n",
    "    return result\n",
    "data = get_base_features(testB_record)\n",
    "data = data[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_list = os.listdir(path+'/Metro_train/')\n",
    "for i in range(0, len(data_list)):\n",
    "    if data_list[i].split('.')[-1] == 'csv':\n",
    "        print(data_list[i], i)\n",
    "        df = pd.read_csv(path+'/Metro_train/' + data_list[i])\n",
    "        df = get_base_features(df)\n",
    "        data = pd.concat([data, df], axis=0, ignore_index=True)\n",
    "    else:\n",
    "        continue\n",
    "data = pd.concat([data, get_base_features(testB_record)], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "#剔除与测试集分布差异大的日期\n",
    "if (test_week!=6)&(test_week!=5):\n",
    "    # 两个方案（1）移除周六周日,1号（2）完全保留同时移除1号 \n",
    "    #df_data_noAbnormal = data.loc[((data.week != 6) & (data.week != 5) & (data.day != 1))].copy() #(1)\n",
    "    df_data_noAbnormal = data.loc[data.day != 1].copy() #(2)\n",
    "elif (test_week==6)|(test_week==5):\n",
    "    # 周六周日作保留 \n",
    "    df_data_noAbnormal = data.loc[((data.week == 6) | (data.week == 5))].copy()\n",
    "\n",
    "df_data_noAbnormal.rename(columns={'time_ten_minutes':'startTime'}, inplace=True)\n",
    "\n",
    "# 保留日期\n",
    "retain_days = list(df_data_noAbnormal.day.unique())\n",
    "print(retain_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 针对线下保留的(用于线下)\n",
    "retain_days = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23]\n",
    "retain_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 23: 18, 24: 19}\n"
     ]
    }
   ],
   "source": [
    "### 保留日+测试集日期\n",
    "days = retain_days + [test_day]\n",
    "\n",
    "days_relative = {}  # 重新计算rank,方便我们后续提取特征\n",
    "for i,d in enumerate(days):\n",
    "    days_relative[d] = i + 1\n",
    "print(days_relative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationIDs = testB_submit['stationID'].unique()\n",
    "times = [] \n",
    "for day in days: \n",
    "    if day < 10:\n",
    "        day_str = '0' + str(day)\n",
    "    else:\n",
    "        day_str = str(day)\n",
    "        \n",
    "    for hour in range(24):\n",
    "        if hour < 10:\n",
    "            hour_str = '0' + str(hour)\n",
    "        else:\n",
    "            hour_str = str(hour)\n",
    "\n",
    "        for minutes in range(6):\n",
    "            if minutes == 0:\n",
    "                minutes_str = '0' + str(minutes)\n",
    "            else:\n",
    "                minutes_str = str(minutes * 10) \n",
    "            times.append('2019-01-' + day_str + ' ' + hour_str +':' + minutes_str + ':00')\n",
    "from itertools import product\n",
    "stationids_by_times = list(product(stationIDs, times))\n",
    "df_data = pd.DataFrame()\n",
    "df_data['stationID'] = np.array(stationids_by_times)[:,0]\n",
    "df_data['startTime'] = np.array(stationids_by_times)[:,1]\n",
    "df_data = df_data.sort_values(['stationID','startTime'])\n",
    "df_data['endTime'] = df_data.groupby('stationID')['startTime'].shift(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filltime(x):\n",
    "    x_split = x.split(' ')[0].split('-')\n",
    "    x_part1_1 = x_split[0] +'-'+x_split[1]+'-'\n",
    "    x_part1_2 = int(x_split[2]) + 1\n",
    "    if x_part1_2 < 10:\n",
    "        x_part1_2 = '0' + str(x_part1_2)\n",
    "    else:\n",
    "        x_part1_2 = str(x_part1_2)\n",
    "        \n",
    "    x_part2 = ' 00:00:00'\n",
    "    return x_part1_1 + x_part1_2 + x_part2\n",
    "\n",
    "df_data.loc[df_data.endTime.isnull(), 'endTime']  = df_data.loc[df_data.endTime.isnull(), 'startTime'].apply(lambda x: filltime(x)) \n",
    "df_data['stationID'] = df_data['stationID'].astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非特殊日期流量\n",
    "df_data = df_data.merge(df_data_noAbnormal, on=['stationID', 'startTime'], how='left')\n",
    "df_data['inNums']  = df_data['inNums'].fillna(0)\n",
    "df_data['outNums'] = df_data['outNums'].fillna(0)\n",
    "\n",
    "# 构造base time        \n",
    "df_data['time']                 = pd.to_datetime(df_data['startTime'])\n",
    "df_data['day' ]                 = df_data['time'].dt.day  \n",
    "df_data['days_relative']        = df_data['day'].map(days_relative)\n",
    "df_data['hour']                 = df_data['time'].dt.hour \n",
    "df_data['minute']               = df_data['time'].dt.minute \n",
    "df_data['week']                 = df_data['time'].dt.dayofweek \n",
    "df_data['ten_minutes_in_day']   = df_data['hour'] * 6 + df_data['time'].dt.minute // 10 \n",
    "del df_data['time']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stationID_fea(df):\n",
    "    df_station               = pd.DataFrame()\n",
    "    df_station['stationID']  = df['stationID'].unique()\n",
    "    df_station               = df_station.sort_values('stationID')\n",
    "    \n",
    "    tmp1  = df.groupby(['stationID'])['deviceID'].nunique().to_frame('stationID_deviceID_nunique').reset_index()\n",
    "    tmp2  = df.groupby(['stationID'])['userID'].nunique().to_frame('stationID_userID_nunique').reset_index()\n",
    "    \n",
    "    df_station = df_station.merge(tmp1,on ='stationID', how='left')\n",
    "    df_station = df_station.merge(tmp2,on ='stationID', how='left')\n",
    "    \n",
    "    for pivot_cols in tqdm_notebook(['payType','hour','days_relative','ten_minutes_in_day']): \n",
    "        \n",
    "        tmp = df.groupby(['stationID',pivot_cols])['deviceID'].count().to_frame('stationID_'+pivot_cols+'_cnt').reset_index()\n",
    "        df_tmp = tmp.pivot(index = 'stationID', columns=pivot_cols, values='stationID_'+pivot_cols+'_cnt')\n",
    "        cols   = ['stationID_'+pivot_cols+'_cnt' + str(col) for col in df_tmp.columns]\n",
    "        df_tmp.columns = cols\n",
    "        df_tmp.reset_index(inplace = True)\n",
    "        df_station = df_station.merge(df_tmp, on ='stationID', how='left')\n",
    "    return df_station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record_2019-01-14.csv 14\n",
      "record_2019-01-15.csv 15\n",
      "record_2019-01-16.csv 16\n",
      "record_2019-01-17.csv 17\n",
      "record_2019-01-18.csv 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be11af9dd0241eeb66363d2aa25ac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_station = get_base_features(testB_record)\n",
    "df_station = df_station[:0]\n",
    "data_list = os.listdir(path+'/Metro_train/')\n",
    "# 提取最近n天符合分布的日期构造station相关特征\n",
    "if (test_week!=6)&(test_week!=5):\n",
    "    #station_day = [17,18,21,22,23,24,25] # 线上\n",
    "    station_day = [14,15,16,17,18] # 线下\n",
    "elif (test_week==6)|(test_week==5):\n",
    "    station_day = [13,19,20]\n",
    "for dtname in data_list:\n",
    "    d = int(dtname[15:17])\n",
    "    if (d in station_day) & (dtname.split('.')[-1] == 'csv'):\n",
    "        print(dtname, d)\n",
    "        df = pd.read_csv(path+'/Metro_train/' + dtname)\n",
    "        df_station = pd.concat([df_station, df], axis=0, ignore_index=True)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "df_station['time']               = pd.to_datetime(df_station['time'])\n",
    "df_station['day']                = df_station['time'].dt.day  \n",
    "df_station['week']               = df_station['time'].dt.dayofweek \n",
    "df_station['hour']               = df_station['time'].dt.hour \n",
    "df_station['ten_minutes_in_day'] = df_station['hour'] * 6 + df_station['time'].dt.minute // 10 \n",
    "df_station['time_ten_minutes']   = df_station['time'].astype(str).apply(lambda x: trans_time_10_minutes(x))\n",
    "df_station['days_relative']      = df_station['day'].map(days_relative)\n",
    "df_station = get_stationID_fea(df_station)\n",
    "\n",
    "def get_stationID_fea(df):\n",
    "    df_station               = pd.DataFrame()\n",
    "    df_station['stationID']  = df['stationID'].unique()\n",
    "    cols = [str(i) for i in range(81)]  \n",
    "    df_station['num_of_join_points'] = df[cols].sum(axis=1).sort_values()\n",
    "    return df_station\n",
    "df_station_metro = get_stationID_fea(Metro_roadMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_before_trans(x,dic_):\n",
    "    if x in dic_.keys():\n",
    "        return dic_[x]\n",
    "    else:\n",
    "        return np.nan\n",
    "def generate_fea_y(df, day, n):\n",
    "    df_feature_y   =  df.loc[df.days_relative == day].copy() \n",
    "    \n",
    "    df_feature_y['tmp_10_minutes'] = df_feature_y['stationID'].values * 1000 + df_feature_y['ten_minutes_in_day'].values\n",
    "    df_feature_y['tmp_hours']      = df_feature_y['stationID'].values * 1000 + df_feature_y['hour'].values\n",
    "    \n",
    "    for i in range(n): # 前n天每一天\n",
    "        d = day - i - 1\n",
    "        df_d = df.loc[df.days_relative == d].copy() # 当天的数据\n",
    "        \n",
    "        # 特征1：过去在该时间段（一样的时间段,10minutes）有多少出入量\n",
    "        df_d['tmp_10_minutes'] = df['stationID'] * 1000 + df['ten_minutes_in_day']  \n",
    "        df_d['tmp_hours']      = df['stationID'] * 1000 + df['hour']\n",
    "        # sum\n",
    "        dic_innums  = df_d.groupby(['tmp_10_minutes'])['inNums'].sum().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_10_minutes'])['outNums'].sum().to_dict()\n",
    "        df_feature_y['_bf_' + str(day -d) + '_innum_10minutes']  =  df_feature_y['tmp_10_minutes'].map(dic_innums).values\n",
    "        df_feature_y['_bf_' + str(day -d) + '_outnum_10minutes'] =  df_feature_y['tmp_10_minutes'].map(dic_outnums).values\n",
    "        \n",
    "        # 特征2：过去在该时间段（小时）有多少出入量\n",
    "        dic_innums  = df_d.groupby(['tmp_hours'])['inNums'].sum().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours'])['outNums'].sum().to_dict()   \n",
    "        df_feature_y['_bf_' + str(day -d) + '_innum_hour']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf_' + str(day -d) + '_outnum_hour'] =  df_feature_y['tmp_hours'].map(dic_outnums).values   \n",
    "        # mean,max,min\n",
    "        dic_innums  = df_d.groupby(['tmp_hours'])['inNums'].mean().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours'])['outNums'].mean().to_dict() \n",
    "        df_feature_y['_bf_' + str(day -d) + '_innum_hour_mean']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf_' + str(day -d) + '_outnum_hour_mean'] =  df_feature_y['tmp_hours'].map(dic_outnums).values\n",
    "        dic_innums  = df_d.groupby(['tmp_hours'])['inNums'].max().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours'])['outNums'].max().to_dict() \n",
    "        df_feature_y['_bf_' + str(day -d) + '_innum_hour_max']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf_' + str(day -d) + '_outnum_hour_max'] =  df_feature_y['tmp_hours'].map(dic_outnums).values\n",
    "        dic_innums  = df_d.groupby(['tmp_hours'])['inNums'].min().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours'])['outNums'].min().to_dict() \n",
    "        df_feature_y['_bf_' + str(day -d) + '_innum_hour_min']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf_' + str(day -d) + '_outnum_hour_min'] =  df_feature_y['tmp_hours'].map(dic_outnums).values\n",
    "        \n",
    "        # 特征3: 上10分钟\n",
    "        df_d['tmp_10_minutes_bf'] = df['stationID'] * 1000 + df['ten_minutes_in_day'] - 1\n",
    "        df_d['tmp_hours_bf']      = df['stationID'] * 1000 + df['hour'] - 1\n",
    "        # sum\n",
    "        dic_innums  = df_d.groupby(['tmp_10_minutes_bf'])['inNums'].sum().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours_bf'])['outNums'].sum().to_dict()\n",
    "        df_feature_y['_bf21_' + str(day -d) + '_innum_10minutes']  =  df_feature_y['tmp_10_minutes'].agg(lambda x: time_before_trans(x,dic_innums)).values\n",
    "        df_feature_y['_bf1_' + str(day -d) + '_outnum_10minutes'] =  df_feature_y['tmp_10_minutes'].agg(lambda x: time_before_trans(x,dic_outnums)).values\n",
    "        \n",
    "        # 特征4： 上个小时情况\n",
    "        dic_innums  = df_d.groupby(['tmp_hours_bf'])['inNums'].sum().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours_bf'])['outNums'].sum().to_dict()   \n",
    "        df_feature_y['_bf1_' + str(day -d) + '_innum_hour']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf1_' + str(day -d) + '_outnum_hour'] =  df_feature_y['tmp_hours'].map(dic_outnums).values\n",
    "        # mean,max,min\n",
    "        dic_innums  = df_d.groupby(['tmp_hours_bf'])['inNums'].mean().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours_bf'])['outNums'].mean().to_dict()   \n",
    "        df_feature_y['_bf1_' + str(day -d) + '_innum_hour_mean']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf1_' + str(day -d) + '_outnum_hour_mean'] =  df_feature_y['tmp_hours'].map(dic_outnums).values\n",
    "        dic_innums  = df_d.groupby(['tmp_hours_bf'])['inNums'].max().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours_bf'])['outNums'].max().to_dict()   \n",
    "        df_feature_y['_bf1_' + str(day -d) + '_innum_hour_max']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf1_' + str(day -d) + '_outnum_hour_max'] =  df_feature_y['tmp_hours'].map(dic_outnums).values\n",
    "        dic_innums  = df_d.groupby(['tmp_hours_bf'])['inNums'].min().to_dict()\n",
    "        dic_outnums = df_d.groupby(['tmp_hours_bf'])['outNums'].min().to_dict()   \n",
    "        df_feature_y['_bf1_' + str(day -d) + '_innum_hour_min']  =  df_feature_y['tmp_hours'].map(dic_innums).values\n",
    "        df_feature_y['_bf1_' + str(day -d) + '_outnum_hour_min'] =  df_feature_y['tmp_hours'].map(dic_outnums).values\n",
    "        \n",
    "    \n",
    "    for col in ['tmp_10_minutes','tmp_hours']:\n",
    "        del df_feature_y[col]\n",
    "        \n",
    "    return df_feature_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 20\n"
     ]
    }
   ],
   "source": [
    "df_feature_y = pd.DataFrame()\n",
    "# 确定范围\n",
    "if (test_week!=6)&(test_week!=5):\n",
    "    #left = 8  # 线上\n",
    "    left = 6   #线下\n",
    "elif (test_week==6)|(test_week==5):\n",
    "    left = 4\n",
    "right = days_relative[test_day]+1\n",
    "print(left,right)\n",
    "# 前n天每一天，以days_relative为准\n",
    "for day in range(left,right):\n",
    "    if i == 0: \n",
    "        df_feature_y = generate_fea_y(df_data, day = day, n=left-1)\n",
    "    else:\n",
    "        df_feature_tmp = generate_fea_y(df_data, day = day, n=left-1)\n",
    "        df_feature_y   = pd.concat([df_feature_y,df_feature_tmp], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['_innum_10minutes','_outnum_10minutes','_innum_hour','_outnum_hour']\n",
    "# # 过去n天的sum,mean\n",
    "for i in range(2,left):\n",
    "    for f in columns:\n",
    "        colname1 = '_bf_'+str(i)+'_'+'days'+f+'_sum'\n",
    "        df_feature_y[colname1] = 0\n",
    "        for d in range(1,i+1):\n",
    "            df_feature_y[colname1] = df_feature_y[colname1] + df_feature_y['_bf_'+str(d) +f]\n",
    "        colname2 = '_bf_'+str(d)+'_'+'days'+f+'_mean'\n",
    "        df_feature_y[colname2] = df_feature_y[colname1] / i\n",
    "        \n",
    "# 过去n天的mean的差分\n",
    "for i in range(2,left): \n",
    "    for f in columns:\n",
    "        colname1 = '_bf_'+str(d)+'_'+'days'+f+'_mean'\n",
    "        colname2 = '_bf_'+str(d)+'_'+'days'+f+'_mean_diff'\n",
    "        df_feature_y[colname2] = df_feature_y[colname1].diff(1)\n",
    "        df_feature_y.loc[(df_feature_y.hour==0)&(df_feature_y.minute==0), colname2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_feature_y.copy()\n",
    "data = data.merge(df_station, on ='stationID', how='left')\n",
    "data = data.merge(df_station_metro, on ='stationID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线下平滑到17号的趋势, 线上平滑到24号趋势\n"
     ]
    }
   ],
   "source": [
    "if (test_week!=6)&(test_week!=5):\n",
    "    print('线下平滑到17号的趋势, 线上平滑到24号趋势')\n",
    "    # 构造小时流量(线上为31)\n",
    "    inNums_hour  = data[data.day!=24].groupby(['stationID','week','day','hour'])['inNums' ].sum().reset_index(name='inNums_hour_sum')\n",
    "    outNums_hour = data[data.day!=24].groupby(['stationID','week','day','hour'])['outNums'].sum().reset_index(name='outNums_hour_sum')\n",
    "    # 合并新构造特征\n",
    "    data = data.merge(inNums_hour , on=['stationID','week','day','hour'], how='left')\n",
    "    data = data.merge(outNums_hour, on=['stationID','week','day','hour'], how='left')\n",
    "    data.fillna(0, inplace=True)\n",
    "    # 提取17号流量（线上为24）\n",
    "    test_nums  = data.loc[data.day==17, ['stationID','ten_minutes_in_day','inNums_hour_sum','outNums_hour_sum']]\n",
    "    test_nums.columns  = ['stationID','ten_minutes_in_day','test_inNums_hour_sum' ,'test_outNums_hour_sum']\n",
    "    # 合并17号流量\n",
    "    data = data.merge(test_nums , on=['stationID','ten_minutes_in_day'], how='left')\n",
    "    # 构造每天与的趋势\n",
    "    data['test_inNums_hour_trend']  = (data['test_inNums_hour_sum'] + 1) / (data['inNums_hour_sum'] + 1 )\n",
    "    data['test_outNums_hour_trend'] = (data['test_outNums_hour_sum'] + 1) / (data['outNums_hour_sum'] + 1)\n",
    "\n",
    "    del data['inNums_hour_sum']\n",
    "    del data['outNums_hour_sum']\n",
    "    del data['test_inNums_hour_sum']\n",
    "    del data['test_outNums_hour_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "线下平滑到17号的趋势, 线上平滑到24号趋势\n",
      "inNums stationID: 0\n",
      "inNums stationID: 1\n",
      "inNums stationID: 2\n",
      "inNums stationID: 3\n",
      "inNums stationID: 4\n",
      "inNums stationID: 5\n",
      "inNums stationID: 6\n",
      "inNums stationID: 7\n",
      "inNums stationID: 8\n",
      "inNums stationID: 9\n",
      "inNums stationID: 10\n",
      "inNums stationID: 11\n",
      "inNums stationID: 12\n",
      "inNums stationID: 13\n",
      "inNums stationID: 14\n",
      "inNums stationID: 15\n",
      "inNums stationID: 16\n",
      "inNums stationID: 17\n",
      "inNums stationID: 18\n",
      "inNums stationID: 19\n",
      "inNums stationID: 20\n",
      "inNums stationID: 21\n",
      "inNums stationID: 22\n",
      "inNums stationID: 23\n",
      "inNums stationID: 24\n",
      "inNums stationID: 25\n",
      "inNums stationID: 26\n",
      "inNums stationID: 27\n",
      "inNums stationID: 28\n",
      "inNums stationID: 29\n",
      "inNums stationID: 30\n",
      "inNums stationID: 31\n",
      "inNums stationID: 32\n",
      "inNums stationID: 33\n",
      "inNums stationID: 34\n",
      "inNums stationID: 35\n",
      "inNums stationID: 36\n",
      "inNums stationID: 37\n",
      "inNums stationID: 38\n",
      "inNums stationID: 39\n",
      "inNums stationID: 40\n",
      "inNums stationID: 41\n",
      "inNums stationID: 42\n",
      "inNums stationID: 43\n",
      "inNums stationID: 44\n",
      "inNums stationID: 45\n",
      "inNums stationID: 46\n",
      "inNums stationID: 47\n",
      "inNums stationID: 48\n",
      "inNums stationID: 49\n",
      "inNums stationID: 50\n",
      "inNums stationID: 51\n",
      "inNums stationID: 52\n",
      "inNums stationID: 53\n",
      "inNums stationID: 54\n",
      "inNums stationID: 55\n",
      "inNums stationID: 56\n",
      "inNums stationID: 57\n",
      "inNums stationID: 58\n",
      "inNums stationID: 59\n",
      "inNums stationID: 60\n",
      "inNums stationID: 61\n",
      "inNums stationID: 62\n",
      "inNums stationID: 63\n",
      "inNums stationID: 64\n",
      "inNums stationID: 65\n",
      "inNums stationID: 66\n",
      "inNums stationID: 67\n",
      "inNums stationID: 68\n",
      "inNums stationID: 69\n",
      "inNums stationID: 70\n",
      "inNums stationID: 71\n",
      "inNums stationID: 72\n",
      "inNums stationID: 73\n",
      "inNums stationID: 74\n",
      "inNums stationID: 75\n",
      "inNums stationID: 76\n",
      "inNums stationID: 77\n",
      "inNums stationID: 78\n",
      "inNums stationID: 79\n",
      "inNums stationID: 80\n",
      "outNums stationID: 0\n",
      "outNums stationID: 1\n",
      "outNums stationID: 2\n",
      "outNums stationID: 3\n",
      "outNums stationID: 4\n",
      "outNums stationID: 5\n",
      "outNums stationID: 6\n",
      "outNums stationID: 7\n",
      "outNums stationID: 8\n",
      "outNums stationID: 9\n",
      "outNums stationID: 10\n",
      "outNums stationID: 11\n",
      "outNums stationID: 12\n",
      "outNums stationID: 13\n",
      "outNums stationID: 14\n",
      "outNums stationID: 15\n",
      "outNums stationID: 16\n",
      "outNums stationID: 17\n",
      "outNums stationID: 18\n",
      "outNums stationID: 19\n",
      "outNums stationID: 20\n",
      "outNums stationID: 21\n",
      "outNums stationID: 22\n",
      "outNums stationID: 23\n",
      "outNums stationID: 24\n",
      "outNums stationID: 25\n",
      "outNums stationID: 26\n",
      "outNums stationID: 27\n",
      "outNums stationID: 28\n",
      "outNums stationID: 29\n",
      "outNums stationID: 30\n",
      "outNums stationID: 31\n",
      "outNums stationID: 32\n",
      "outNums stationID: 33\n",
      "outNums stationID: 34\n",
      "outNums stationID: 35\n",
      "outNums stationID: 36\n",
      "outNums stationID: 37\n",
      "outNums stationID: 38\n",
      "outNums stationID: 39\n",
      "outNums stationID: 40\n",
      "outNums stationID: 41\n",
      "outNums stationID: 42\n",
      "outNums stationID: 43\n",
      "outNums stationID: 44\n",
      "outNums stationID: 45\n",
      "outNums stationID: 46\n",
      "outNums stationID: 47\n",
      "outNums stationID: 48\n",
      "outNums stationID: 49\n",
      "outNums stationID: 50\n",
      "outNums stationID: 51\n",
      "outNums stationID: 52\n",
      "outNums stationID: 53\n",
      "outNums stationID: 54\n",
      "outNums stationID: 55\n",
      "outNums stationID: 56\n",
      "outNums stationID: 57\n",
      "outNums stationID: 58\n",
      "outNums stationID: 59\n",
      "outNums stationID: 60\n",
      "outNums stationID: 61\n",
      "outNums stationID: 62\n",
      "outNums stationID: 63\n",
      "outNums stationID: 64\n",
      "outNums stationID: 65\n",
      "outNums stationID: 66\n",
      "outNums stationID: 67\n",
      "outNums stationID: 68\n",
      "outNums stationID: 69\n",
      "outNums stationID: 70\n",
      "outNums stationID: 71\n",
      "outNums stationID: 72\n",
      "outNums stationID: 73\n",
      "outNums stationID: 74\n",
      "outNums stationID: 75\n",
      "outNums stationID: 76\n",
      "outNums stationID: 77\n",
      "outNums stationID: 78\n",
      "outNums stationID: 79\n",
      "outNums stationID: 80\n"
     ]
    }
   ],
   "source": [
    "if (test_week!=6)&(test_week!=5):\n",
    "    print('线下平滑到17号的趋势, 线上平滑到24号趋势')\n",
    "    data['inNums_online']  = data['inNums']\n",
    "    data['outNums_online'] = data['outNums']\n",
    "    for sid in range(0,81):\n",
    "        print('inNums stationID:', sid)\n",
    "        for d in range(2,17):#线上（2,24）\n",
    "            inNums = data.loc[(data.stationID==sid)&(data.day==d),'inNums']\n",
    "            trend  = data.loc[(data.stationID==sid)&(data.day==d),'test_inNums_hour_trend']\n",
    "            data.loc[(data.stationID==sid)&(data.day==d),'inNums_online'] = trend.values*(inNums.values+1)-1\n",
    "        for d in range(18,19):#线上（25,26）\n",
    "            inNums = data.loc[(data.stationID==sid)&(data.day==d),'inNums']\n",
    "            trend  = data.loc[(data.stationID==sid)&(data.day==d),'test_inNums_hour_trend']\n",
    "            data.loc[(data.stationID==sid)&(data.day==d),'inNums_online'] = trend.values*(inNums.values+1)-1\n",
    "\n",
    "    for sid in range(0,81):\n",
    "        print('outNums stationID:', sid)\n",
    "        for d in range(2,17):#线上（2,24）\n",
    "            outNums = data.loc[(data.stationID==sid)&(data.day==d),'outNums']\n",
    "            trend   = data.loc[(data.stationID==sid)&(data.day==d),'test_outNums_hour_trend']\n",
    "            data.loc[(data.stationID==sid)&(data.day==d),'outNums_online'] = trend.values*(outNums.values+1)-1\n",
    "        for d in range(18,19):#线上（25,26）\n",
    "            outNums = data.loc[(data.stationID==sid)&(data.day==d),'outNums']\n",
    "            trend   = data.loc[(data.stationID==sid)&(data.day==d),'test_outNums_hour_trend']\n",
    "            data.loc[(data.stationID==sid)&(data.day==d),'outNums_online'] = trend.values*(outNums.values+1)-1\n",
    "\n",
    "    # 后处理\n",
    "    data.loc[data.inNums_online <0 , 'inNums_online' ] = 0\n",
    "    data.loc[data.outNums_online<0 , 'outNums_online'] = 0\n",
    "\n",
    "    data['inNums']  = data['inNums_online']\n",
    "    data['outNums'] = data['outNums_online']\n",
    "    del data['inNums_online']\n",
    "    del data['outNums_online']\n",
    "    del data['test_inNums_hour_trend']\n",
    "    del data['test_outNums_hour_trend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = [f for f in data.columns if f not in ['startTime','endTime','inNums','outNums']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "### all data\n",
    "data.fillna(0, inplace=True)\n",
    "all_data = data[data.day!=test_day]\n",
    "X_data = all_data[all_columns].values\n",
    "\n",
    "test  = data[data.day==test_day]\n",
    "X_test = test[all_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb'):\n",
    "    oof = np.zeros(X.shape[0])\n",
    "    predictions = np.zeros(X_test.shape[0])\n",
    "    scores = []\n",
    "    for fold_n, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            trn_data = lgb.Dataset(X[trn_idx], y[trn_idx])\n",
    "            val_data = lgb.Dataset(X[val_idx], y[val_idx])\n",
    "            clf = lgb.train(params,\n",
    "                            trn_data,\n",
    "                            num_boost_round=20000,\n",
    "                            valid_sets=[trn_data,val_data],\n",
    "                            valid_names=['train','valid'],\n",
    "                            early_stopping_rounds=200,\n",
    "                            verbose_eval=400,\n",
    "                            )\n",
    "            oof[val_idx] = clf.predict(X[val_idx], num_iteration=clf.best_iteration)\n",
    "            predictions += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "       \n",
    "        \n",
    "        scores.append(mean_absolute_error(oof[val_idx], y[val_idx]))\n",
    "        \n",
    "        \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    return oof, predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'num_leaves': 63,\n",
    "              'min_data_in_leaf': 25, \n",
    "              'objective':'regression_l1',\n",
    "              'max_depth': -1,\n",
    "              'learning_rate': 0.05,\n",
    "              'min_child_samples': 20,\n",
    "              'boosting': 'gbdt',\n",
    "              'feature_fraction': 0.9,\n",
    "              'bagging_fraction': 0.9 ,\n",
    "              'bagging_seed': 11,\n",
    "              'metric': 'mae',\n",
    "              'lambda_l1': 0.1,\n",
    "              'nthread': 50,\n",
    "              'verbosity': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Apr  2 14:29:47 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.89668\tvalid's l1: 10.4657\n",
      "[800]\ttrain's l1: 9.70433\tvalid's l1: 10.3345\n",
      "[1200]\ttrain's l1: 9.51376\tvalid's l1: 10.2384\n",
      "[1600]\ttrain's l1: 9.3813\tvalid's l1: 10.1846\n",
      "[2000]\ttrain's l1: 9.34047\tvalid's l1: 10.1644\n",
      "[2400]\ttrain's l1: 9.18476\tvalid's l1: 10.1209\n",
      "[2800]\ttrain's l1: 9.0552\tvalid's l1: 10.0996\n",
      "[3200]\ttrain's l1: 8.90394\tvalid's l1: 10.0716\n",
      "[3600]\ttrain's l1: 8.82935\tvalid's l1: 10.0649\n",
      "Early stopping, best iteration is:\n",
      "[3568]\ttrain's l1: 8.84187\tvalid's l1: 10.0639\n",
      "Fold 1 started at Tue Apr  2 14:34:26 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.89319\tvalid's l1: 10.4953\n",
      "[800]\ttrain's l1: 9.79471\tvalid's l1: 10.4335\n",
      "[1200]\ttrain's l1: 9.66448\tvalid's l1: 10.3623\n",
      "[1600]\ttrain's l1: 9.57809\tvalid's l1: 10.3265\n",
      "[2000]\ttrain's l1: 9.55171\tvalid's l1: 10.3138\n",
      "[2400]\ttrain's l1: 9.50003\tvalid's l1: 10.297\n",
      "[2800]\ttrain's l1: 9.38573\tvalid's l1: 10.256\n",
      "[3200]\ttrain's l1: 9.31673\tvalid's l1: 10.2259\n",
      "[3600]\ttrain's l1: 9.17258\tvalid's l1: 10.1885\n",
      "[4000]\ttrain's l1: 9.0866\tvalid's l1: 10.1598\n",
      "[4400]\ttrain's l1: 8.92979\tvalid's l1: 10.1354\n",
      "[4800]\ttrain's l1: 8.88666\tvalid's l1: 10.1275\n",
      "Early stopping, best iteration is:\n",
      "[4781]\ttrain's l1: 8.88674\tvalid's l1: 10.1274\n",
      "Fold 2 started at Tue Apr  2 14:41:01 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.92219\tvalid's l1: 10.2548\n",
      "[800]\ttrain's l1: 9.80965\tvalid's l1: 10.1835\n",
      "[1200]\ttrain's l1: 9.73296\tvalid's l1: 10.1448\n",
      "[1600]\ttrain's l1: 9.6582\tvalid's l1: 10.105\n",
      "[2000]\ttrain's l1: 9.59533\tvalid's l1: 10.077\n",
      "[2400]\ttrain's l1: 9.5456\tvalid's l1: 10.0659\n",
      "[2800]\ttrain's l1: 9.53071\tvalid's l1: 10.0621\n",
      "[3200]\ttrain's l1: 9.52319\tvalid's l1: 10.0606\n",
      "[3600]\ttrain's l1: 9.48528\tvalid's l1: 10.0509\n",
      "[4000]\ttrain's l1: 9.45224\tvalid's l1: 10.0404\n",
      "[4400]\ttrain's l1: 9.33462\tvalid's l1: 10.0096\n",
      "[4800]\ttrain's l1: 9.18154\tvalid's l1: 9.98868\n",
      "[5200]\ttrain's l1: 8.97205\tvalid's l1: 9.95436\n",
      "[5600]\ttrain's l1: 8.86794\tvalid's l1: 9.92506\n",
      "[6000]\ttrain's l1: 8.27264\tvalid's l1: 9.87333\n",
      "Early stopping, best iteration is:\n",
      "[6012]\ttrain's l1: 8.23488\tvalid's l1: 9.87025\n",
      "Fold 3 started at Tue Apr  2 14:49:56 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.87332\tvalid's l1: 10.2042\n",
      "[800]\ttrain's l1: 9.76773\tvalid's l1: 10.1139\n",
      "[1200]\ttrain's l1: 9.74944\tvalid's l1: 10.1006\n",
      "[1600]\ttrain's l1: 9.71545\tvalid's l1: 10.0835\n",
      "[2000]\ttrain's l1: 9.67288\tvalid's l1: 10.0633\n",
      "[2400]\ttrain's l1: 9.61156\tvalid's l1: 10.0322\n",
      "[2800]\ttrain's l1: 9.48738\tvalid's l1: 9.97226\n",
      "[3200]\ttrain's l1: 9.4706\tvalid's l1: 9.96674\n",
      "[3600]\ttrain's l1: 9.45626\tvalid's l1: 9.96323\n",
      "[4000]\ttrain's l1: 9.35883\tvalid's l1: 9.93225\n",
      "Early stopping, best iteration is:\n",
      "[4135]\ttrain's l1: 9.34658\tvalid's l1: 9.92744\n",
      "Fold 4 started at Tue Apr  2 14:56:14 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.93281\tvalid's l1: 10.2099\n",
      "[800]\ttrain's l1: 9.87298\tvalid's l1: 10.1721\n",
      "[1200]\ttrain's l1: 9.73031\tvalid's l1: 10.0737\n",
      "[1600]\ttrain's l1: 9.53233\tvalid's l1: 9.98236\n",
      "[2000]\ttrain's l1: 9.46664\tvalid's l1: 9.95883\n",
      "[2400]\ttrain's l1: 9.44186\tvalid's l1: 9.95376\n",
      "[2800]\ttrain's l1: 9.41775\tvalid's l1: 9.93961\n",
      "[3200]\ttrain's l1: 9.25412\tvalid's l1: 9.90343\n",
      "[3600]\ttrain's l1: 9.1699\tvalid's l1: 9.88344\n",
      "[4000]\ttrain's l1: 9.1535\tvalid's l1: 9.87794\n",
      "[4400]\ttrain's l1: 9.09474\tvalid's l1: 9.86076\n",
      "[4800]\ttrain's l1: 8.93443\tvalid's l1: 9.854\n",
      "[5200]\ttrain's l1: 8.66074\tvalid's l1: 9.82447\n",
      "Early stopping, best iteration is:\n",
      "[5378]\ttrain's l1: 8.40884\tvalid's l1: 9.80891\n",
      "Fold 5 started at Tue Apr  2 15:04:12 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.91108\tvalid's l1: 10.5217\n",
      "[800]\ttrain's l1: 9.78516\tvalid's l1: 10.4359\n",
      "[1200]\ttrain's l1: 9.75091\tvalid's l1: 10.4125\n",
      "[1600]\ttrain's l1: 9.6994\tvalid's l1: 10.3777\n",
      "[2000]\ttrain's l1: 9.64239\tvalid's l1: 10.3512\n",
      "[2400]\ttrain's l1: 9.58395\tvalid's l1: 10.3304\n",
      "[2800]\ttrain's l1: 9.46586\tvalid's l1: 10.2928\n",
      "[3200]\ttrain's l1: 9.44672\tvalid's l1: 10.2886\n",
      "[3600]\ttrain's l1: 9.42069\tvalid's l1: 10.2756\n",
      "[4000]\ttrain's l1: 9.11615\tvalid's l1: 10.2324\n",
      "[4400]\ttrain's l1: 8.57877\tvalid's l1: 10.1685\n",
      "[4800]\ttrain's l1: 8.1705\tvalid's l1: 10.1367\n",
      "Early stopping, best iteration is:\n",
      "[4658]\ttrain's l1: 8.21756\tvalid's l1: 10.1346\n",
      "Fold 6 started at Tue Apr  2 15:10:43 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.95779\tvalid's l1: 10.4462\n",
      "[800]\ttrain's l1: 9.81513\tvalid's l1: 10.3517\n",
      "[1200]\ttrain's l1: 9.71239\tvalid's l1: 10.2855\n",
      "[1600]\ttrain's l1: 9.68502\tvalid's l1: 10.2705\n",
      "[2000]\ttrain's l1: 9.64333\tvalid's l1: 10.2573\n",
      "[2400]\ttrain's l1: 9.56694\tvalid's l1: 10.221\n",
      "[2800]\ttrain's l1: 8.82932\tvalid's l1: 10.0783\n",
      "[3200]\ttrain's l1: 8.08131\tvalid's l1: 10.0196\n",
      "[3600]\ttrain's l1: 7.79431\tvalid's l1: 9.99816\n",
      "Early stopping, best iteration is:\n",
      "[3689]\ttrain's l1: 7.67988\tvalid's l1: 9.99115\n",
      "Fold 7 started at Tue Apr  2 15:16:25 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.96267\tvalid's l1: 10.3471\n",
      "[800]\ttrain's l1: 9.73824\tvalid's l1: 10.2167\n",
      "[1200]\ttrain's l1: 9.57481\tvalid's l1: 10.1607\n",
      "[1600]\ttrain's l1: 9.54511\tvalid's l1: 10.1445\n",
      "[2000]\ttrain's l1: 9.52518\tvalid's l1: 10.1385\n",
      "[2400]\ttrain's l1: 9.51936\tvalid's l1: 10.1357\n",
      "[2800]\ttrain's l1: 9.50911\tvalid's l1: 10.1326\n",
      "[3200]\ttrain's l1: 9.41032\tvalid's l1: 10.1052\n",
      "[3600]\ttrain's l1: 9.31249\tvalid's l1: 10.0721\n",
      "[4000]\ttrain's l1: 9.23324\tvalid's l1: 10.0449\n",
      "[4400]\ttrain's l1: 9.16231\tvalid's l1: 10.0243\n",
      "[4800]\ttrain's l1: 9.08039\tvalid's l1: 10.0088\n",
      "[5200]\ttrain's l1: 9.06645\tvalid's l1: 10.0074\n",
      "Early stopping, best iteration is:\n",
      "[5056]\ttrain's l1: 9.0676\tvalid's l1: 10.007\n",
      "Fold 8 started at Tue Apr  2 15:23:46 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.93758\tvalid's l1: 10.3304\n",
      "[800]\ttrain's l1: 9.76881\tvalid's l1: 10.2422\n",
      "[1200]\ttrain's l1: 9.66639\tvalid's l1: 10.2106\n",
      "[1600]\ttrain's l1: 9.58541\tvalid's l1: 10.1866\n",
      "[2000]\ttrain's l1: 9.53767\tvalid's l1: 10.1709\n",
      "[2400]\ttrain's l1: 9.42996\tvalid's l1: 10.1325\n",
      "[2800]\ttrain's l1: 9.21735\tvalid's l1: 10.1032\n",
      "Early stopping, best iteration is:\n",
      "[2746]\ttrain's l1: 9.22492\tvalid's l1: 10.0997\n",
      "Fold 9 started at Tue Apr  2 15:27:53 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 9.92632\tvalid's l1: 10.0302\n",
      "[800]\ttrain's l1: 9.75163\tvalid's l1: 9.93063\n",
      "[1200]\ttrain's l1: 9.63015\tvalid's l1: 9.87932\n",
      "[1600]\ttrain's l1: 9.48199\tvalid's l1: 9.82603\n",
      "[2000]\ttrain's l1: 9.42376\tvalid's l1: 9.80464\n",
      "[2400]\ttrain's l1: 9.39223\tvalid's l1: 9.79088\n",
      "[2800]\ttrain's l1: 9.38446\tvalid's l1: 9.78885\n",
      "[3200]\ttrain's l1: 9.35449\tvalid's l1: 9.78215\n",
      "[3600]\ttrain's l1: 9.2693\tvalid's l1: 9.76247\n",
      "[4000]\ttrain's l1: 9.01876\tvalid's l1: 9.7314\n",
      "[4400]\ttrain's l1: 8.51676\tvalid's l1: 9.68142\n",
      "[4800]\ttrain's l1: 8.26325\tvalid's l1: 9.67329\n",
      "[5200]\ttrain's l1: 8.04279\tvalid's l1: 9.65852\n",
      "Early stopping, best iteration is:\n",
      "[5062]\ttrain's l1: 8.1453\tvalid's l1: 9.656\n",
      "CV mean score: 9.9686, std: 0.1468.\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=10, shuffle=True, random_state=2019)\n",
    "oof_in , predictions_in , scores_in  = train_model(X_data , X_test, all_data['inNums'], params=lgb_params, folds=folds, model_type='lgb')\n",
    "test['pred_inNums']= predictions_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 started at Tue Apr  2 15:36:24 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 11.1352\tvalid's l1: 11.4772\n",
      "[800]\ttrain's l1: 10.9051\tvalid's l1: 11.3414\n",
      "[1200]\ttrain's l1: 10.8421\tvalid's l1: 11.3114\n",
      "[1600]\ttrain's l1: 10.7408\tvalid's l1: 11.2797\n",
      "[2000]\ttrain's l1: 10.7014\tvalid's l1: 11.2612\n",
      "[2400]\ttrain's l1: 10.6827\tvalid's l1: 11.2514\n",
      "[2800]\ttrain's l1: 10.642\tvalid's l1: 11.241\n",
      "[3200]\ttrain's l1: 10.6295\tvalid's l1: 11.2353\n",
      "Early stopping, best iteration is:\n",
      "[3318]\ttrain's l1: 10.628\tvalid's l1: 11.2345\n",
      "Fold 1 started at Tue Apr  2 15:40:34 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 11.196\tvalid's l1: 11.6705\n",
      "[800]\ttrain's l1: 10.9898\tvalid's l1: 11.5191\n",
      "[1200]\ttrain's l1: 10.9103\tvalid's l1: 11.4756\n",
      "[1600]\ttrain's l1: 10.86\tvalid's l1: 11.4579\n",
      "[2000]\ttrain's l1: 10.7865\tvalid's l1: 11.4364\n",
      "[2400]\ttrain's l1: 10.7058\tvalid's l1: 11.4216\n",
      "[2800]\ttrain's l1: 10.4837\tvalid's l1: 11.3866\n",
      "[3200]\ttrain's l1: 10.1751\tvalid's l1: 11.362\n",
      "Early stopping, best iteration is:\n",
      "[3296]\ttrain's l1: 10.0399\tvalid's l1: 11.3452\n",
      "Fold 2 started at Tue Apr  2 15:44:58 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[400]\ttrain's l1: 11.1683\tvalid's l1: 11.5314\n",
      "[800]\ttrain's l1: 11.0025\tvalid's l1: 11.4474\n",
      "[1200]\ttrain's l1: 10.893\tvalid's l1: 11.3905\n",
      "[1600]\ttrain's l1: 10.8193\tvalid's l1: 11.3639\n",
      "[2000]\ttrain's l1: 10.7315\tvalid's l1: 11.3353\n",
      "[2400]\ttrain's l1: 10.6653\tvalid's l1: 11.3111\n",
      "[2800]\ttrain's l1: 10.5298\tvalid's l1: 11.2654\n",
      "[3200]\ttrain's l1: 10.3071\tvalid's l1: 11.2234\n",
      "[3600]\ttrain's l1: 10.177\tvalid's l1: 11.2068\n",
      "[4000]\ttrain's l1: 10.0359\tvalid's l1: 11.1897\n"
     ]
    }
   ],
   "source": [
    "folds = KFold(n_splits=10, shuffle=True, random_state=2019)\n",
    "oof_out , predictions_out , scores_out  = train_model(X_data , X_test, all_data['outNums'], params=lgb_params, folds=folds, model_type='lgb')\n",
    "test['pred_outNums']= predictions_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = test.sort_values(['stationID','hour','minute'])\n",
    "sub = pd.read_csv(path + '/Metro_testB/testB_submit_2019-01-27.csv')\n",
    "sub['hour']         =  sub['startTime'].apply(lambda x: int(x[11:13]))\n",
    "sub['minute']       =  sub['startTime'].apply(lambda x: int(x[14:15]+'0'))\n",
    "sub['inNums']       =  tst['inNums'].values\n",
    "sub['outNums']      =  tst['outNums'].values\n",
    "sub['pred_inNums']       =  tst['pred_inNums'].values\n",
    "sub['pred_outNums']      =  tst['pred_outNums'].values\n",
    "# 结果修正\n",
    "sub.loc[sub.pred_inNums <0 , 'pred_inNums' ] = 0\n",
    "sub.loc[sub.pred_outNums<0 , 'pred_outNums'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.616510321672436"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(np.append(sub['inNums'],sub['outNums']),np.append(sub['pred_inNums'],sub['pred_outNums']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.781789300384307"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(sub['inNums'],sub['pred_inNums'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (PySpark)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
